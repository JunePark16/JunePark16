{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunePark16/JunePark16/blob/main/hw3_s3d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c391297a",
      "metadata": {
        "id": "c391297a"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2dc99769",
      "metadata": {
        "id": "2dc99769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fb7074-b592-4771-9267-6858b80084f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1e622f99",
      "metadata": {
        "id": "1e622f99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308994d6-4423-4e0e-e271-f8543a3c3e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MLVU_HW3/MLVU_HW3/S3D\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Change directory to where this file is located\n",
        "\"\"\"\n",
        "%cd /content/drive/MyDrive/MLVU_HW3/MLVU_HW3/S3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "34dd41c4",
      "metadata": {
        "id": "34dd41c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c129f4b5-f380-432e-f8d5-61b83ff50f9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nUnzip the UCF11 zip file.\\n- Run this cell only once.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\"\n",
        "Unzip the UCF11 zip file.\n",
        "- Run this cell only once.\n",
        "\"\"\"\n",
        "# !unzip -n ./UCF11.zip -d ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b017c7fc",
      "metadata": {
        "id": "b017c7fc"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9e6d3650",
      "metadata": {
        "id": "9e6d3650"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bb00e243",
      "metadata": {
        "id": "bb00e243"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import more modules you need\n",
        "\"\"\"\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv3D, ReLU, BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f4562ac8",
      "metadata": {
        "id": "f4562ac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1815131d-8600-4cec-fb48-63bd47f50e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num_CPUs:1, List:[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Num_GPUs:1, List:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Make sure your runtime type is GPU!\n",
        "\"\"\"\n",
        "cpus = tf.config.list_physical_devices('CPU')\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print('Num_CPUs:{}, List:{}'.format(len(cpus), cpus))\n",
        "print('Num_GPUs:{}, List:{}'.format(len(gpus), gpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99bd1e34",
      "metadata": {
        "id": "99bd1e34"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "583b470f",
      "metadata": {
        "id": "583b470f"
      },
      "outputs": [],
      "source": [
        "def bytes_feature(value):\n",
        "    \"\"\"\n",
        "    Returns a bytes_list from a string / byte.\n",
        "\n",
        "    Do NOT modify this method.\n",
        "    \"\"\"\n",
        "    if not isinstance(value, list):\n",
        "        value = [value]\n",
        "        # bytes_feature 의 인자에는 list 가 들어가도록 변환\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
        "\n",
        "def int64_feature(value):\n",
        "    \"\"\"\n",
        "    Returns a float_list from a float / double.\n",
        "\n",
        "    Do NOT modify this method.\n",
        "    \"\"\"\n",
        "    if not isinstance(value, list):\n",
        "        value = [value]\n",
        "        # int64_feature 의 인자에는 list 가 들어가도록 \n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
        "\n",
        "def float_feature(value):\n",
        "    \"\"\"\n",
        "    Returns a float_list from a float / double.\n",
        "\n",
        "    Do NOT modify this method.\n",
        "    \"\"\"\n",
        "    if not isinstance(value, list):\n",
        "        value = [value]\n",
        "        # float_feature 의 인자에는 list 가 들어가도록\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
        "    \n",
        "def plot_frames(videos, labels, grid_width, grid_height, figure_width=15, figure_height=5, y_hats=None):\n",
        "    \"\"\"\n",
        "    Plots videos and labels.\n",
        "\n",
        "    Do NOT modify this method.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(grid_height, grid_width)\n",
        "    fig.set_size_inches(figure_width, figure_height)\n",
        "    video_idx = 0\n",
        "    imgs = []\n",
        "    for i in range(0, grid_height):\n",
        "        for j in range(0, grid_width):\n",
        "            video = videos[video_idx]\n",
        "            label = labels[video_idx]\n",
        "            title_color = 'k'\n",
        "            if y_hats is None:\n",
        "                label_idx = int(label)\n",
        "            else:\n",
        "                label_idx = int(y_hats[video_idx])\n",
        "                if int(labels[video_idx]) != label_idx:\n",
        "                    title_color = 'r'\n",
        "            label = InputPipelineBuilder.LABELS[label_idx]\n",
        "            ax[i][j].axis('off')\n",
        "            ax[i][j].set_title(label, color=title_color)\n",
        "            \n",
        "            for frame_idx, frame in enumerate(video):\n",
        "                img = ax[i][j].imshow(frame, animated=True, aspect='auto')\n",
        "                if video_idx==0:\n",
        "                    imgs.append([img])\n",
        "                else:\n",
        "                    imgs[frame_idx].extend([img])\n",
        "            ani = animation.ArtistAnimation(fig, imgs, interval=150, blit=False)\n",
        "            video_idx += 1\n",
        "    plt.close(fig)\n",
        "    os.makedirs('./temp', exist_ok=True)\n",
        "    ani.save(f'./temp/frames.gif', writer='pillow', fps=10)\n",
        "    display(HTML(ani.to_html5_video()))\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"\n",
        "    Plots training history.\n",
        "    \n",
        "    Do NOT modify this method.\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32df3045",
      "metadata": {
        "id": "32df3045"
      },
      "source": [
        "# UCF11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cc650270",
      "metadata": {
        "id": "cc650270"
      },
      "outputs": [],
      "source": [
        "class InputPipelineBuilder:\n",
        "    \"\"\"\n",
        "    UCF11 Input Pipeline\n",
        "    - 1. Contains configuration and information about the inputs.\n",
        "    - 2. Splits the dataset into train/valid/test subets.\n",
        "    - 3. Load videos and sample frames to use, followed by preprocessing.\n",
        "    - 4. Store the sampled frames as TFRecord files.\n",
        "    - 5. Returns TF dataset objects built directly from the TFRecord files.\n",
        "    \"\"\"\n",
        "    # pipeline configurations \n",
        "    FRAME_SIZE = (56,56)      \n",
        "    NUM_FRAMES_PER_VIDEO = 16   # video 당 16개의 프레임을 보겠다\n",
        "    NUM_VIDEOS_PER_RECORD = 64  \n",
        "    SUBSETS = ['train', 'valid', 'test']\n",
        "    LABELS = ['basketball', 'biking', 'diving', 'golf_swing',\n",
        "              'horse_riding', 'soccer_juggling', 'swing', 'tennis_swing',\n",
        "              'trampoline_jumping', 'volleyball_spiking', 'walking']\n",
        "    NUM_LABELS = 11\n",
        "    INPUT_SHAPE = [NUM_FRAMES_PER_VIDEO, *FRAME_SIZE, 3]    # *FRAME_SIZE 는 tuple 로 component 를 받는다.\n",
        "    \n",
        "    def __init__(self, dataset_dir, tfrecord_dir):\n",
        "        \"\"\"\n",
        "        Loads and splits UCF11 to train, validation, and test sets.\n",
        "\n",
        "        Do NOT modify this method.\n",
        "        \"\"\"\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.tfrecord_dir = tfrecord_dir\n",
        "        self.x = None   # self.x 에는 train/validation/test set 의 data\n",
        "        self.y = None   # self.y 에는 train/validataion/test set 의 label\n",
        "        \n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Print the input pipeline configuration and data subsets.\n",
        "        \n",
        "        Do NOT modify this method.\n",
        "        \"\"\"\n",
        "        info = ''\n",
        "        info += f'Labels: {InputPipelineBuilder.LABELS}\\n'\n",
        "        info += f'Dimension to extract per video: {[InputPipelineBuilder.NUM_FRAMES_PER_VIDEO, *InputPipelineBuilder.FRAME_SIZE, 3]}\\n'\n",
        "        info += 'Dataset splitted: '\n",
        "        if self.x:\n",
        "            info += '/'.join(InputPipelineBuilder.SUBSETS)\n",
        "            info += ' [' + '|'.join([str(len(self.x[subset])) for subset in InputPipelineBuilder.SUBSETS]) + ']'\n",
        "        return info\n",
        "    \n",
        "    def split_dataset(self, valid_size=0.2, test_size=0.1, random_state=None):\n",
        "        \"\"\"\n",
        "        Read video lists and split them into subsets\n",
        "        \n",
        "        Do NOT modify this method.\n",
        "        \"\"\"\n",
        "        videos = []\n",
        "        labels = []\n",
        "        for label_int, label_str in enumerate(InputPipelineBuilder.LABELS):\n",
        "            # InputPipelineBuilder.LABELS 에는 11개의 label\n",
        "            path = os.path.join(self.dataset_dir, label_str)\n",
        "            temp = glob(path+'/**/*.mpg')\n",
        "            videos.extend(temp)    # extend 는 iterable element 를 넣는다.\n",
        "            labels.extend([label_int,] * len(temp))\n",
        "            # if label_int == 0 :\n",
        "            #     print(f'len(temp) = {len(temp)}')\n",
        "        x_train, x_test, y_train, y_test   = train_test_split(videos, labels, test_size=test_size, stratify=labels,\n",
        "                                                              shuffle=True, random_state=random_state)\n",
        "        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=valid_size, stratify=y_train,\n",
        "                                                              shuffle=True, random_state=random_state)\n",
        "        self.x = {'train':x_train, 'valid':x_valid, 'test':x_test}  # self.x 는 video 를 담은 딕셔너리\n",
        "        self.y = {'train':y_train, 'valid':y_valid, 'test':y_test}  # selfl.y 는 label 을 담은 딕셔너리\n",
        "        \n",
        "    def convert_dataset_to_tfrecord(self, subset='train'):\n",
        "        \"\"\"\n",
        "        Load and store videos in TFRecord format.\n",
        "        \n",
        "        Do NOT modify this method.\n",
        "        \"\"\"\n",
        "        assert subset in InputPipelineBuilder.SUBSETS, 'subset supported: {}'.format(InputPipelineBuilder.SUBSETS)\n",
        "        # InputPipilineBuilder.SUBSETS 는 train, valid, test 중 하나\n",
        "        videos = self.x[subset]\n",
        "        labels = self.y[subset]\n",
        "        tfrecord_dir = os.path.join(self.tfrecord_dir, subset) # train, valid, test\n",
        "        files = glob(tfrecord_dir+'/*')\n",
        "        for file in files:\n",
        "            os.remove(file)\n",
        "            # 우선 tfrecord_dir + subset 폴더 초기화\n",
        "        os.makedirs(tfrecord_dir, exist_ok=True)\n",
        "            # tfrecord_dir + subset 폴더가 존재해도 오류 발생시키지 X\n",
        "        for i in tqdm(range(0, len(videos), InputPipelineBuilder.NUM_VIDEOS_PER_RECORD)):   \n",
        "            # InputPipelineBuilder.NUM_VIDEOS_PER_RECORD : 64 -> record(파일) 하나 당 64개의 비디오를 저장\n",
        "            # tqdm 은 process 진행 바     \n",
        "            tfrecords_file_path = os.path.join(tfrecord_dir, 'tfr_'+str(i))\n",
        "            with tf.io.TFRecordWriter(tfrecords_file_path) as record_writer:\n",
        "                upto = min(i+InputPipelineBuilder.NUM_VIDEOS_PER_RECORD, len(videos))\n",
        "                for video, label in zip(videos[i:upto], labels[i:upto]):\n",
        "                        example = self.encode_tfrecord(video, label)    ## example 은 byte sequence\n",
        "                        record_writer.write(example.SerializeToString())    # record 파일을 2진 문자열로 직렬화\n",
        "        print(\"TFRecord Conversion DONE\\n\")\n",
        "        \n",
        "    def convert_tfrecord_to_tfdataset(self, subset='train'):\n",
        "        \"\"\"\n",
        "        Creates a tf.dataset from TFRecord files.\n",
        "\n",
        "        Do NOT modify this method.\n",
        "        \"\"\"\n",
        "        assert subset in InputPipelineBuilder.SUBSETS, 'subset supported: {}'.format(InputPipelineBuilder.SUBSETS)\n",
        "        # InputPipelineBuilder.SUBSETS : train, valid, test 중 하나\n",
        "        tfrecord_files = glob(os.path.join(self.tfrecord_dir, subset + '/*'))\n",
        "        dataset = tf.data.TFRecordDataset(tfrecord_files).map(lambda x: self.decode_tfrecord(x))    # 2진 문자열을 video 로 변환\n",
        "        return dataset  \n",
        "        \n",
        "    def load_video(self, video):\n",
        "        \"\"\"\n",
        "        Loads a video from a given path and samples a certain number of frames at equal distance.\n",
        "        Preprocsses each frame while sampling.\n",
        "\n",
        "        Question (a)\n",
        "        - Use cv2.VideoCapture to access the contents of the video at the given path.\n",
        "        - Computes the frame indices to sample using ...\n",
        "        - the number of the total frames achieved with the 'get' method of cv2.VideoCapture.\n",
        "        - the number of frames to sample is stored as a class variable: InputPipelineBuilder.NUM_FRAMES_PER_VIDEO\n",
        "        - Read each frame at these indices and apply self.preprocess \n",
        "        - Release the capture object at the end.\n",
        "\n",
        "        - Refer to OpenCV VideoCapture Documentation for further information!\n",
        "\n",
        "        Inputs\n",
        "        - video: a file path to a video.\n",
        "        Returns\n",
        "        - frames: a numpy array of frames extracted from the video.\n",
        "        \"\"\"\n",
        "        frames=[]#\n",
        "\n",
        "        cap = cv2.VideoCapture(video)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        # for i in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
        "        dist = frame_count/InputPipelineBuilder.NUM_FRAMES_PER_VIDEO\n",
        "        for i in range(frame_count) :\n",
        "            # if i % int(cap.get(cv2.CAP_PROP_FRAME_COUNT))/InputPipelineBuilder.NUM_FRAMES_PER_VIDEO == 0 :\n",
        "            if i % dist == 0 :\n",
        "                retval, frame = cap.read()\n",
        "                frame = self.preprocess(frame)    ## ?? frame = preprocess(frame)\n",
        "                frames.append(frame)\n",
        "        cap.release()\n",
        "        # cv2.destroyAllwindows() \n",
        "        return np.asarray(frames) #\n",
        "    \n",
        "    def preprocess(self, frame):\n",
        "        \"\"\"\n",
        "        Preprocess the given frame.\n",
        "\n",
        "        Question (a)\n",
        "        - Use OpenCV(cv2) methods.\n",
        "        - 1. Convert the color format from BGR to RGB; OpenCV reads frames in the BGR format.\n",
        "        - 2. Resize the frame to InputPipelineBuilder.FRAME_SIZE.\n",
        "        - 3. Normalize.\n",
        "        \n",
        "        Inputs\n",
        "        - frame\n",
        "        Returns\n",
        "        - frame: a frame preprocssed.\n",
        "        \"\"\"\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # 1.\n",
        "        frame = cv2.resize(frame, InputPipelineBuilder.FRAME_SIZE)  # 2.\n",
        "        frame = cv2.normalize(frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX) ## beta = 255? , dtype = cv2.CV_32F?\n",
        "        return frame\n",
        "        \n",
        "    def encode_tfrecord(self, video, label):\n",
        "        # video 랑 label 을 tf.train.Example 로 변환 -> byte sequence\n",
        "        # output 은 byte sequence 로 이루어저 잇고 이것들로 TFRecord file 을 만듬\n",
        "\n",
        "        # encode 는 기존 데이터를 byte 로 바꾼다는 뜻\n",
        "\n",
        "        # encoding 할 video 은 np.array (->bytes)\n",
        "        # encoding 할 label 은 int64 type\n",
        "        \"\"\"\n",
        "        Convert a set of video and label as tf.train.Example.\n",
        "        An example contains a byte sequence of one data point and these examples comprise a TFRecord file.\n",
        "\n",
        "        Question (b)\n",
        "        - Load frames from a video and use methods in Utils section in order to ...\n",
        "        - Convert frames to byte_features. You will need to convert np.array frames to a byte array beforehand.\n",
        "        - Convert label to int64_features.\n",
        "\n",
        "        - Use these features to initialize tf.train.Features and tf.train.Example\n",
        "        - Refer to TensorFlow TFRecord Tutorial for further information!\n",
        "\n",
        "        Inputs\n",
        "        - video: a video path\n",
        "        - label: an action label\n",
        "        Returns\n",
        "        - example: tf.train.Example\n",
        "        \"\"\"\n",
        "        frames = self.load_video(video) # frames : nparray\n",
        "        feature = {\n",
        "            'frames' : bytes_feature(frames.tobytes()),\n",
        "            'label' : int64_feature(label)\n",
        "        }\n",
        "        example = tf.train.Example(features=tf.train.Features(feature=feature))  ## ??\n",
        "        return example  # example 은 byte sequence\n",
        "\n",
        "    def decode_tfrecord(self, tfrecord):\n",
        "        # decode 는 byte 를 기존 데이터 포맷으로 바꾼다.\n",
        "        \"\"\"\n",
        "        Decode a tfrecord item.\n",
        "        You need to specify how many bytes to read from this byte sequence per feature to retrieve a data point.\n",
        "\n",
        "        Question (b)\n",
        "        - Use tf.io.FixedLenFeature([], 'type of feature')\n",
        "        - Use tf.io.parse_single_example, tf.io.decode_raw\n",
        "            - Use InputPipelineBuilder.INPUT_SHAPE to reshape after decoding frames.\n",
        "            - Use tf.cast to cast label to tf.int32 after decoding label.    \n",
        "                ## tf.int32???\n",
        "\n",
        "        - Refer to TensorFlow TFRecord Tutorial for further information!\n",
        "\n",
        "        Inputs\n",
        "        - tfrecord\n",
        "        Returns\n",
        "        - frames\n",
        "        - labels\n",
        "        \"\"\"\n",
        "        feature_desc = {\n",
        "            'frames' : tf.io.FixedLenFeature([], tf.string ),\n",
        "            'label' : tf.io.FixedLenFeature([], tf.int64 )\n",
        "        }\n",
        "        # need to specify how many bytes to read from this byte sequence per feature to retrieve a data point.\n",
        "\n",
        "        ## label : tf.string -> tf.int64\n",
        "        ## label 을 tf.string(:bytes) 으로 변환하면 안될 것 같은데..\n",
        "        \n",
        "        frames = tf.io.parse_single_example(tfrecord , feature_desc)['frames']\n",
        "        frames = tf.io.decode_raw(frames, tf.float32) ## tf.float32 -> tf.int32 -> tf.float32\n",
        "        frames = tf.reshape(frames, InputPipelineBuilder.INPUT_SHAPE)\n",
        "\n",
        "        label = tf.io.parse_single_example(tfrecord, feature_desc)['label']\n",
        "        label = tf.io.decode_raw(label, tf.float32)\n",
        "        label=tf.cast(label, tf.int32)  \n",
        "        # Use tf.cast to cast label to tf.int32 after decoding label.\n",
        "        return frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9b091636",
      "metadata": {
        "id": "9b091636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1030d121-7344-4196-b3c2-ce394e28ff1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: ['basketball', 'biking', 'diving', 'golf_swing', 'horse_riding', 'soccer_juggling', 'swing', 'tennis_swing', 'trampoline_jumping', 'volleyball_spiking', 'walking']\n",
            "Dimension to extract per video: [16, 56, 56, 3]\n",
            "Dataset splitted: train/valid/test [1149|288|160]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Builds a UCF11 input pipeline\n",
        "\"\"\"\n",
        "input_pipeline_builder = InputPipelineBuilder(dataset_dir='./UCF11', tfrecord_dir='./tfrecord')\n",
        "input_pipeline_builder.split_dataset(valid_size=0.2, test_size=0.1)\n",
        "print(input_pipeline_builder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "175e4a7f",
      "metadata": {
        "id": "175e4a7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429c75e3-4896-4928-9eca-e48adf6ad16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/18 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Convert videos into TFRecord files\n",
        "You don't need to run this cell more than once unless TFRecord files need updates.\n",
        "\"\"\"\n",
        "##### 딱 한 번만 실행 #####\n",
        "use_gpu = True #False\n",
        "cpus = tf.config.list_logical_devices('CPU')\n",
        "gpus = tf.config.list_logical_devices('GPU')\n",
        "with tf.device(gpus[0].name if use_gpu else cpus[0].name):\n",
        "    input_pipeline_builder.convert_dataset_to_tfrecord(subset='train')\n",
        "    input_pipeline_builder.convert_dataset_to_tfrecord(subset='valid')\n",
        "    input_pipeline_builder.convert_dataset_to_tfrecord(subset='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "2e0c8811",
      "metadata": {
        "id": "2e0c8811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "a5625f33-e26f-44fe-cd41-bc36f5481eea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-9a7029784369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalid_ds_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pipeline_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tfrecord_to_tfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_ds_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pipeline_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tfrecord_to_tfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds_preprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplot_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2843\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2845\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2846\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Key: label.  Data types don't match. Data type: int64 but expected type: string\n\t [[{{node ParseSingleExample_1/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]"
          ]
        }
      ],
      "source": [
        "train_ds_preprocessed = input_pipeline_builder.convert_tfrecord_to_tfdataset(subset='train')\n",
        "valid_ds_preprocessed = input_pipeline_builder.convert_tfrecord_to_tfdataset(subset='valid')\n",
        "test_ds_preprocessed = input_pipeline_builder.convert_tfrecord_to_tfdataset(subset='test')\n",
        "videos, labels = next(iter(train_ds_preprocessed.shuffle(100).batch(16).take(1)))\n",
        "plot_frames(videos, labels, 8, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "754f0e98",
      "metadata": {
        "id": "754f0e98"
      },
      "source": [
        "# S3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e23cc2",
      "metadata": {
        "id": "29e23cc2"
      },
      "outputs": [],
      "source": [
        "def sep_conv3d_bn(input_tensor, kernel_size, filters, strides=1, padding='same', separate=False):\n",
        "    \"\"\"\n",
        "    Builds a separable Conv3D+BN layer.\n",
        "    \n",
        "    Question (c)\n",
        "    - Use ReLU to each Conv3D+BN for activation\n",
        "    - Use l2 regularizer for each Conv3D.\n",
        "    - for separated Conv3Ds, apply the same number of filters for both.\n",
        "    - when separate=False, this is just a normal Conv3D.\n",
        "\n",
        "    Inputs\n",
        "    - input_tensor\n",
        "    - kernel_size: an integer to indicate kernel_size  \n",
        "    - filters: an integer to indicate the number of filters    -> integer && list 가능\n",
        "    - strides: an integer to indicate strides\n",
        "    Returns\n",
        "    - output_tensor\n",
        "    \"\"\"\n",
        "    if separate:        \n",
        "        # Spatial Conv : 1x3x3\n",
        "        output_tensor = tf.keras.layers.Conv3D(\n",
        "            filters = filters[0],\n",
        "            kernel_size = (1, kernel_size, kernel_size),\n",
        "            activation = 'relu',\n",
        "            kernel_regularizer = 'l2')(input_tensor)\n",
        "        # Temporal Conv : 3x1x1\n",
        "        output_tensor = tf.keras.layers.Conv3D(\n",
        "            filters = filters[1],\n",
        "            kernel_size = (kernel_size, 1, 1),\n",
        "            activation = 'relu',\n",
        "            kernel_regularizer = 'l2')(output_tensor)\n",
        "        \n",
        "    else:\n",
        "        # Spatio-temporal Conv\n",
        "        output_tensor = tf.keras.layers.Conv3D(\n",
        "            filters = filters,\n",
        "            kernel_size = (kernel_size, kernel_size, kernel_size),\n",
        "            activation = 'relu',\n",
        "            kernel_regularizer = 'l2')(input_tensor)\n",
        "    \n",
        "    # BN\n",
        "    output_tensor = tf.keras.layers.BatchNormalization()(output_tensor)    ## ??\n",
        "    return output_tensor\n",
        "\n",
        "def sep_inception(input_tensor, filters1, filters2, filters3, filters4):\n",
        "    \"\"\"\n",
        "    Builds a separable Inception block.\n",
        "    \n",
        "    Question (c)\n",
        "    - Refer to the figure 2 for the overall architecture.\n",
        "    - Use sep_conv3d_bn above.\n",
        "\n",
        "    Inputs\n",
        "    - input_tensor\n",
        "    - filters1: an integer to indicate the number of filters\n",
        "    - filters2: a list of integers to indicate the number of filters\n",
        "    - filters3: a list of integers to indicate the number of filters\n",
        "    - filters4: an integer to indicate the number of filters\n",
        "    Returns\n",
        "    - output_tensor\n",
        "    \"\"\"\n",
        "    # 1x1x1 Conv\n",
        "    conv1 = sep_conv3d_bn(input_tensor = input_tensor,\n",
        "                          kernel_size = 1,\n",
        "                          filters = filters1)\n",
        "    # 1x1x1 Conv + 3x3x3 Conv Separated\n",
        "    conv2 = sep_conv3d_bn(input_tensor = input_tensor,\n",
        "                          kernel_size = 1,\n",
        "                          filters = filters2[0])\n",
        "    conv2 = sep_conv3d_bn(input_tensor = conv2,\n",
        "                          kernel_size = 3,\n",
        "                          filters = filters2[1],\n",
        "                          separate = True)\n",
        "    # 1x1x1 Conv + 3x3x3 Conv Separated\n",
        "    conv3 = sep_conv3d_bn(input_tensor = input_tensor,\n",
        "                          kernel_size = 1,\n",
        "                          filters = filters3[0])\n",
        "    conv3 = sep_conv3d_bn(input_tensor = conv2,\n",
        "                          kernel_size = 3,\n",
        "                          filters = filters3[1],\n",
        "                          separate = True)\n",
        "    # 3x3x3 MaxPool + 1x1x1 Conv\n",
        "    conv4 = tf.keras.layers.MaxPooling3D(pool_size=(3,3,3))(input_tensor)\n",
        "    conv4 = sep_conv3d_bn(input_tensor = conv4,\n",
        "                          kernel_size = 3,\n",
        "                          filters = filters4)\n",
        "    # Concatenation\n",
        "    output_tensor = tf.keras.layers.Concatenate(axis=0)([conv1, conv2, conv3, conv4])\n",
        "    return output_tensor\n",
        "    \n",
        "def S3D(input_shape):\n",
        "    \"\"\"\n",
        "    Builds a S3D model.\n",
        "\n",
        "    Question (d)\n",
        "    - Refer to the figure 2 for the overall architecture.\n",
        "    - You can try different layers for fc layers!\n",
        "\n",
        "    Inputs\n",
        "    - input_shape: the shape of the input image.\n",
        "    Returns\n",
        "    - a TF Keras Model model of S3D\n",
        "    \"\"\"\n",
        "    # input layer\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    # stem\n",
        "    x = sep_conv3d_bn(input_tensor, kernel_size = 3, filters = 192, strides = 2)\n",
        "    x = tf.keras.layers.MaxPooling3D(pool_size=(1,2,2))(x)\n",
        "    # separable inception block 1 + max pooling\n",
        "    x = sep_inception(input_tensor = x, \n",
        "                      filter1 = 64,\n",
        "                      filter2 = [96,128],\n",
        "                      filter3 = [16,32],\n",
        "                      filter4 = 32)\n",
        "    x = tf.keras.layers.MaxPooling3D(pool_size=(2,2,2))(x)\n",
        "    # separable inception block 2 + max pooling\n",
        "    x = sep_inception(input_tensor = x, \n",
        "                      filter1 = 64,\n",
        "                      filter2 = [96,128],\n",
        "                      filter3 = [16,32],\n",
        "                      filter4 = 32)\n",
        "    x = tf.keras.layers.MaxPooling3D(pool_size=(2,2,2))(x)\n",
        "    # 1x1x1 Conv\n",
        "    x = sep_conv3d_bn(x, kernel_size = 1, filters = 256)\n",
        "\n",
        "    ### fc layers -- tf.keras.layers 추가\n",
        "    x = tf.keras.layers.Flatten()(x)    \n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(256, kernel_regularizer=l2(0.001))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(InputPipelineBuilder.NUM_LABELS, kernel_regularizer=l2(0.001), activation='softmax')(x)\n",
        "    \n",
        "    return tf.keras.Model(input_tensor, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hcdWOHDumTvZ",
      "metadata": {
        "id": "hcdWOHDumTvZ"
      },
      "outputs": [],
      "source": [
        "### \n",
        "# Question (e)\n",
        "# Train your S3D to achieve test accuracy 80% and above.\n",
        "# You can try or add other training options such as SGD or callbacks to schedule learning rates if you want.\n",
        "### "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79914535",
      "metadata": {
        "id": "79914535"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training Args\n",
        "- modify as you want\n",
        "- set load_weights=True to load the previous best checkpoint.\n",
        "\"\"\"\n",
        "batch_size = 16\n",
        "num_epochs = 15\n",
        "learning_rate = 0.0005\n",
        "load_weights = False\n",
        "print_summary = False\n",
        "checkpoint_path = './ckpts/s3d_ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e86b36",
      "metadata": {
        "id": "65e86b36"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utilize TF dataset methods to expedite I/O.\n",
        "\"\"\"\n",
        "train_ds_preprocessed = input_pipeline_builder.convert_tfrecord_to_tfdataset(subset='train')\n",
        "valid_ds_preprocessed = input_pipeline_builder.convert_tfrecord_to_tfdataset(subset='valid')\n",
        "test_ds_preprocessed = input_pipeline_builder.convert_tfrecord_to_tfdataset(subset='test')\n",
        "\n",
        "train_ds, train_steps = train_ds_preprocessed.repeat().shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE),\\\n",
        "                        len(input_pipeline_builder.x['train'])//batch_size\n",
        "valid_ds, valid_steps = valid_ds_preprocessed.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE),\\\n",
        "                        len(input_pipeline_builder.x['valid'])//batch_size\n",
        "test_ds, test_steps = test_ds_preprocessed.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE),\\\n",
        "                        len(input_pipeline_builder.x['test'])//batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cea5ed4",
      "metadata": {
        "id": "3cea5ed4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "s3d = S3D(InputPipelineBuilder.INPUT_SHAPE)\n",
        "s3d.compile(optimizer, loss, metrics=metric)\n",
        "if load_weights:\n",
        "    s3d.load_weights(checkpoint_path)\n",
        "if print_summary:\n",
        "    s3d.summary()\n",
        "\n",
        "callbacks = []\n",
        "callbacks.append(tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, save_weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720b71c5",
      "metadata": {
        "id": "720b71c5"
      },
      "outputs": [],
      "source": [
        "history = s3d.fit(train_ds,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  validation_data=valid_ds,\n",
        "                  validation_steps=valid_steps,\n",
        "                  callbacks=callbacks,\n",
        "                  epochs=num_epochs,\n",
        "                  verbose=1)\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e45d87a",
      "metadata": {
        "id": "2e45d87a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Measure test accuracy\n",
        "\"\"\"\n",
        "s3d.evaluate(test_ds, steps=test_steps)\n",
        "videos, labels = next(iter(test_ds.shuffle(buffer_size=200).take(1)))\n",
        "y_hats = np.argmax(s3d.predict(videos), axis=1)\n",
        "plot_frames(videos, labels, 8, 2, y_hats=y_hats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xl_Cimjq29VJ",
      "metadata": {
        "id": "xl_Cimjq29VJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw3_s3d.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}